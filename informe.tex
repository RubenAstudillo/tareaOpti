\documentclass[letterpaper]{article}

\usepackage[letterpaper]{geometry}
\usepackage[log-declarations=false]{xparse}
\usepackage[quiet]{fontspec}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{enumerate}
\usepackage{polyglossia}

\setdefaultlanguage{spanish}

\begin{document}
\title{Optimizacion No Lineal: Tarea 1}
\author{Ruben Astudillo \\ 201021009-k}
\date{\today}
\maketitle

\section*{Pregunta 1}
\noindent Para los ejemplos \(1\) y \(2\) de la ayudantia varie el
parametro \(\alpha\) y compare sus resultados. Para que valores de
\(\alpha\) no se tiene convergencia. Utilizar los metodos de maximo
descenso y de Newton.
\newline

Se analisaran individualmente cada una de las combinaciones. Para
desarrollar estos se realizaron en \texttt{octave} en vez de
\texttt{Matlab} por lo que pueden haber incompatibilidades que se me
hayan pasado por alto.ó €¿

\subsection*{Ejemplo 1 con maximo descenso}
\noindent Nuestra funcion corresponde a
\[ f(x,y) = -10 + x^2 + y^2\]
\[ \nabla f (x,y) = [2 x , 2 y ] \]
Es claro que esta obtiene su minimo en \((0,0)\), por lo que podremos
comparar con respecto a este que tan lejos de la solucion termino el
algoritmo. El codigo se encuentra en la primera mitad del archivo
\texttt{pregunta\_1.m} donde se define \texttt{primer\_ej}
correspondiente a la funcion anterior. Dado que el punto inicial esta
dado en el ejemplo en \(x_0 = (2.5, 1.5)\) se respeta este valor como
partida, ademas utilizamos una toleracian de \(10e4\)
\begin{verbatim}
ans = Maximo descenso ejemplo1: x0 = (1.50, 2.50)
ans = alpha = 0.10, iteraciones =    51, final = (0.00, 0.00)
ans = alpha = 0.20, iteraciones =    23, final = (0.00, 0.00)
ans = alpha = 0.30, iteraciones =    13, final = (0.00, 0.00)
ans = alpha = 0.50, iteraciones =     2, final = (0.00, 0.00)
ans = alpha = 0.70, iteraciones =    13, final = (0.00, 0.00)
ans = alpha = 0.75, iteraciones =    17, final = (0.00, 0.00)
ans = alpha = 0.80, iteraciones =    23, final = (0.00, 0.00)
ans = alpha = 0.85, iteraciones =    32, final = (-0.00, -0.00)
ans = alpha = 0.90, iteraciones =    51, final = (0.00, 0.00)
ans = alpha = 0.95, iteraciones =   106, final = (-0.00, -0.00)
ans = alpha = 0.98, iteraciones =   270, final = (-0.00, -0.00)
ans = alpha = 1.00, iteraciones = 10000, final = (-1.50, -2.50)
ans = alpha = 1.20, iteraciones =  2108, final = (-Inf, NaN)
ans = ultimo caso hay divergencia. Convergencia si alpha < 1
\end{verbatim}
Vemos que a medida que el \(\alpha\) tiende a \(1\) le cantidad de
iteraciones va llegando al limite y despues de este las soluciones se
alejan del minimo \((0,0)\). Por tanto diremos que para \(\alpha < 1\)
el algoritmo de maximo descendo converge.

\subsection*{Ejemplo 1 con metodo de newton}
\noindent Similarmente que en la seccion anterior dados los mismos
parametros de inicio y toleracia junto con el hessiano de nuestra funcion
\[ H(x,y) =
  \begin{pmatrix}
    2 & 0 \\
    0 & 2
  \end{pmatrix}
  \qquad
  H^{-1}(x,y) =
  \begin{pmatrix}
    0.5 & 0 \\
    0 & 0.5
  \end{pmatrix}
\]
Se obtienen los siguientes resultados
\begin{verbatim}
ans = newton ejemplo1: x0 = (1.50, 2.50)
ans = alpha = 1.00, iteraciones =     2, final = (0.00, 0.00)
ans = alpha = 1.20, iteraciones =     8, final = (-0.00, -0.00)
ans = alpha = 1.40, iteraciones =    13, final = (0.00, 0.00)
ans = alpha = 1.60, iteraciones =    23, final = (0.00, 0.00)
ans = alpha = 1.80, iteraciones =    51, final = (0.00, 0.00)
ans = alpha = 1.90, iteraciones =   106, final = (-0.00, -0.00)
ans = alpha = 1.95, iteraciones =   215, final = (0.00, 0.00)
ans = alpha = 1.99, iteraciones =  1093, final = (0.00, 0.00)
ans = alpha = 2.00, iteraciones = 10000, final = (-1.50, -2.50)
ans = alpha = 2.10, iteraciones =  7433, final = (NaN, NaN)
ans = ultimo caso hay divergencia. Convergencia si alpha < 2
\end{verbatim}
Por lo que concluimos que para \(\alpha < 2\) hay convergencia.

\subsection*{Ejemplo 2 con maximo descenso}
\noindent Nuestra funcion cambia a
\[ f(x,y) = -10 + 3 x^2 + y^2 \]
\[ \nabla f (x,y) = [6 x , 2 y ] \]
Nuestras parametro siguen siendo iguales
\begin{verbatim}
ans = maximo descenso ejemplo2: x0 = (1.50, 2.50)
ans = alpha = 0.20000, iteraciones =    23, final = (0.00, 0.00)
ans = alpha = 0.25000, iteraciones =    18, final = (-0.00, 0.00)
ans = alpha = 0.30000, iteraciones =    53, final = (0.00, 0.00)
ans = alpha = 0.32000, iteraciones =   138, final = (-0.00, 0.00)
ans = alpha = 0.33000, iteraciones =   566, final = (-0.00, 0.00)
ans = alpha = 0.33333, iteraciones = 10000, final = (-1.50, 0.00)
ans = alpha = 0.35000, iteraciones =  7428, final = (NaN, 0.00)
ans = ultimo caso hay divergencia. Convergencia si alpha < (1/3)
\end{verbatim}
Por lo que concluimos que para \(\alpha < \frac 1 3 \) hay convergencia.

\subsection*{Ejemplo 2 con metodo de Newton}
\noindent Necesitamos calcular el hessiano de nuestra funcion
\[ H(x,y) =
  \begin{pmatrix}
    6 & 0 \\
    0 & 2
  \end{pmatrix}
  \qquad
  H^{-1}(x,y) =
  \begin{pmatrix}
    \frac{1}{6} & 0 \\
    0 & 0.5
  \end{pmatrix}
\]
\begin{verbatim}
ans = newton ejemplo2: x0 = (1.50, 2.50)
ans = alpha = 1.40, iteraciones =    14, final = (-0.00, -0.00)
ans = alpha = 1.60, iteraciones =    24, final = (-0.00, -0.00)
ans = alpha = 1.80, iteraciones =    53, final = (0.00, 0.00)
ans = alpha = 1.90, iteraciones =   111, final = (0.00, 0.00)
ans = alpha = 1.95, iteraciones =   227, final = (0.00, 0.00)
ans = alpha = 1.98, iteraciones =   573, final = (0.00, 0.00)
ans = alpha = 1.99, iteraciones =  1150, final = (-0.00, -0.00)
ans = alpha = 2.00, iteraciones = 10000, final = (-1.50, -2.50)
ans = alpha = 2.10, iteraciones =  7427, final = (Inf, NaN)
ans = ultimo caso hay divergencia. Convergencia si alpha < 2
\end{verbatim}
por tanto se concluye que para \(\alpha < 2\) se converge.

\section*{Pregunta 2}
\noindent Minimice la funcion
\[ p(x,y) = \frac{1}{1 + (x-2)^2 + 2 (y+1)^2} + \frac{0.7}{1 + (x+1)^2 +
    2 (y - 2)^2 } \]
Utilizando el metodo de \emph{Leverberg-Marquardt}. Intente con distintos
valores de \(\alpha, \lambda\). Pruebe ademas con los valores iniciales
\((1.5, -2.5), (1.5, 2.5), (0,0), (1,1), (-3,-3)\). Compare los resultados.

\section*{Pregunta 3}
\noindent Minimice la funcion de \emph{Rosenbrock}
\[ f(x,y) = 100 (y - x^2)^2 + (1 - x)^2 \]
Utilizando los tres metodos vistos anteriormente y compare resultados
\newline

Los codigos estan en el archivo \texttt{pregunta\_3.m} con un ejemplo de
los resultados en \texttt{respuesta\_3.txt}. Es claro que el punto
minimizante de esta funcion corresponde a \((1,1)\), asi que tomamos un
punto inicial \(x_0 = (2.5, 3.5)\) no tan lejos de este.

\subsection*{Maximo descenso}
Este corresponde a la primera seccion en el archivo
\texttt{respuesta\_3.txt}. Para utilizar este necesitamos el gradiente de
la funcion
\[ \nabla f (x,y) = [-400 x (y - x^2) - 2 (1-x),\ 200 (y-x^2)] \]
Con esto, los resultados son todos de la forma
\begin{verbatim}
ans = x1 = (Inf, Inf), alpha = X , iter = 7, valor = NaN
\end{verbatim}
con \(X \in [0.1 \dots 1]\). Para cada uno de estos valores este
metodo no converge. La presencia de NaN como valor y dado que la forma de
la funcion es una parabola nos da a intuir que este metodo llega a algun
valor infinito el cual no le permite superar la septima iteracion.

\subsection*{Metodo de Newton}
Corresponde a la segunda seccion del archivo. Necesitamos el Hessiano de
la funcion
\[ H (x,y) =
  \begin{pmatrix}
    -400 y - 1200 x^2 + 2 & -400 x \\
    -400 x & 200
  \end{pmatrix}
\]
Solo para algunos valores de \(\alpha\) en \([0.4 ,\ 0.5]\) este metodo
converge
\begin{verbatim}
ans = x1 = (-7.57, 57.26), alpha = 0.40, iter = 10000, valor = 74.40
ans = x1 = (0.96, 0.91), alpha = 0.41, iter = 10000, valor = 0.01
ans = x1 = (1.00, 1.00), alpha = 0.42, iter = 10000, valor = 0.00
ans = x1 = (1.07, 1.14), alpha = 0.43, iter = 10000, valor = 0.00
ans = x1 = (13.09, 171.12), alpha = 0.44, iter = 10000, valor = 147.56
ans = x1 = (-24.40, 595.08), alpha = 0.46, iter = 10000, valor = 649.03
ans = x1 = (1.26, 1.58), alpha = 0.47, iter = 10000, valor = 0.09
ans = x1 = (1.00, 1.00), alpha = 0.48, iter = 10000, valor = 0.00
ans = x1 = (-1.11, 0.72), alpha = 0.49, iter = 10000, valor = 31.37
ans = x1 = (4.69, 21.90), alpha = 0.50, iter = 10000, valor = 13.97
\end{verbatim}
Fuera de este rango diverge. Notese que siempre llega al maximo de
iteraciones, si subiera el limite de estas en todas excepto \(3\)
llegarian a este. Podemos concluir que converge si y solo si se escogen
valores correctos de \(\alpha\).

\subsection*{Metodo de Levenberg}
La ultima seccion del archivo. Este metodo para todos los valores de
\(\alpha \in [0.1,\ 0.5]\) y \(\lambda \in [0.1,\ 0.5]\) probados
converge al valor \((1,1)\) dicho anteriormente
\begin{verbatim}
ans = x1 = (1.00, 1.00), lambda = 0.20, alpha = 0.40, iter = 57, valor = 0.00
ans = x1 = (1.00, 1.00), lambda = 0.27, alpha = 0.30, iter = 80, valor = 0.00
ans = x1 = (1.00, 1.00), lambda = 0.30, alpha = 0.50, iter = 50, valor = 0.00
ans = x1 = (1.00, 1.00), lambda = 0.37, alpha = 0.30, iter = 88, valor = 0.00
ans = x1 = (1.00, 1.00), lambda = 0.40, alpha = 0.43, iter = 63, valor = 0.00
ans = x1 = (1.00, 1.00), lambda = 0.47, alpha = 0.33, iter = 87, valor = 0.00
ans = x1 = (1.00, 1.00), lambda = 0.50, alpha = 0.43, iter = 69, valor = 0.00
\end{verbatim}
Algo interesante a considerar es la consistencia de la cantidad de
iteraciones que le toma llegar al optimo. Podemos considerar que con
mayor certeza este metodo para este problema converge.
\end{document}
